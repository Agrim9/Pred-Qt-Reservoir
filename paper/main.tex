%% This is a skeleton file demonstrating the use of IEEEtran.cls (requires IEEEtran.cls version 1.8a or later) with an IEEE conference paper.
%%
%% Modified by Khan Reaz( kahn.reaz@ieee.org)
%% Support sites:
%% http://www.ieee.org/

%%***********************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or implied; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk and can modify as s/he wants.

%%***********************************************************

\def\b0{{\bf 0}}
\def\ba{{\bf a}}
\def\bb{{\bf b}}
\def\bc{{\bf c}}
\def\bd{{\bf d}}
\def\be{{\bf e}}
\def\bg{{\bf g}}
\def\bh{{\bf h}}
\def\bi{{\bf i}}
\def\bj{{\bf j}}
\def\bk{{\bf k}}
\def\bl{{\bf l}}
\def\bm{{\bf m}}
\def\bn{{\bf n}}
\def\bo{{\bf o}}
\def\bp{{\bf p}}
\def\bq{{\bf q}}
\def\br{{\bf r}}
\def\bs{{\bf s}}
\def\bt{{\bf t}}
\def\bu{{\bf u}}
\def\bv{{\bf v}}
\def\bw{{\bf w}}
\def\bx{{\bf x}}
\def\by{{\bf y}}
\def\bz{{\bf z}}


\def\bSigma{{\bf \Sigma}}
\def\bA{{\bf A}}
\def\bB{{\bf B}}
\def\bC{{\bf C}}
\def\bD{{\bf D}}
\def\bE{{\bf E}}
\def\bF{{\bf F}}
\def\bG{{\bf G}}
\def\bH{{\bf H}}
\def\bI{{\bf I}}
\def\bJ{{\bf J}}
\def\bK{{\bf K}}
\def\bL{{\bf L}}
\def\bM{{\bf M}}
\def\bN{{\bf N}}
\def\bO{{\bf O}}
\def\bP{{\bf P}}
\def\bQ{{\bf Q}}
\def\bR{{\bf R}}
\def\bS{{\bf S}}
\def\bT{{\bf T}}
\def\bU{{\bf U}}
\def\bV{{\bf V}}
\def\bW{{\bf W}}
\def\bX{{\bf X}}
\def\bY{{\bf Y}}
\def\bZ{{\bf Z}}


%package list
\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
\let\labelindent\relax
\usepackage{enumitem}
\usepackage{fleqn}
\usepackage{cite}
\usepackage{graphicx}
\usepackage[varg]{newtxmath}
\graphicspath{ {images/} }
\usepackage{pdfpages}	
\usepackage{wrapfig}
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage{lettrine}
\usepackage{amsmath}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{float}
\usepackage[font={footnotesize}]{caption}
\usepackage[numbers,sort,square,compress]{natbib}
\usepackage[para]{footmisc}
% \usepackage{parskip}
% \setlength{\parskip}{0.02\baselineskip}

% \fancypagestyle{plain}{
%   \fancyhf{} % sets both header and footer to nothing
% \renewcommand{\headrulewidth}{0pt}
%   \fancyhead[C]{2018 International Conference on Indoor Positioning and Indoor Navigation (IPIN), 24-27 September 2018, Nantes, France}% Right header

% }
\pagestyle{plain}% Set page style to plain.

% \pagestyle{fancyplain}
% \fancyhf{}
% \renewcommand{\headrulewidth}{0pt}
% \fancyhead[C]{2018 International Conference on Indoor Positioning and Indoor Navigation (IPIN), 24-27 September 2018, Nantes, France}
\DeclareMathOperator*{\argmin}{argmin}
\begin{document}
%Here goes the title

\title{Predictive Quantization for MIMO-OFDM SVD Precoders using Reservoir Computing Framework}


%Authors List

 \author{\authorblockN{Agrim Gupta, Pranav Sankhe, Kumar Appaiah, Manoj Gopalkrishnan}
 \
 \authorblockA{Department of Electrical Engineering, Indian Institute of Technology Bombay\\
 \{agrim,pranavs,akumar,manojg\}@ee.iitb.ac.in}
% \thanks{Parts of this work was supported by the Bharti Centre for Communication in
% IIT Bombay, and the Visvesvaraya
% PhD Scheme of Ministry of Electronics \& Information Technology,
% Government of India (implemented by the Digital India Corporation).
% }}
}
\maketitle

\thispagestyle{plain}
%Main body starts



  % \noindent We consider problem of quantization and interpolation of
  % time and frequency varying precoding matrices in wireless MIMO
  % systems. Knowledge of precoder matrices at the transmitter can We first establish a result comparing the performance of
  % quantization and interpolation attempted directly over the Stiefel
  % manifold rather than over the Unitary manifold, as has been the
  % primary approach thus far. We propose a predictive quantization
  % algorithm to improve upon the quantization metric by exploiting both
  % time and frequency correlations in a constructed frequency hopping
  % scenario. Building upon these, we finally propose a joint
  % time-frequency interpolating algorithm to estimate the precoders
  % which were not fed back. We demonstrate significant improvements in
  % both BER curve and log-sum rate capacity over the existing
  % approaches.
\begin{abstract}

  % Precoding transmissions in wireless MIMO systems is essential to
  % enable optimal utilization of the spatial degrees of
  % freedom. However, communicating the precoding matrices from the
  % receiver is challenging, owing to large feedback requirements. Past
  % work has shown that predictive quantization in time, as well as
  % interpolation over frequency can be used to reconstruct the
  % precoders over a wide band, although these techniques have not been
  % used jointly. We propose both a predictive quantization as well as a
  % joint time-frequency interpolation strategy for precoding matrices
  % over the Stiefel manifold. The key insight that we use is that
  % local tangent spaces in the manifold permit effective combination of
  % both temporal and frequency domain information for more accurate
  % precoder reconstruction. Simulations reveal that we obtain a
  % significant improvement in achievable rate as well as BER reduction
  % when compared to existing strategies.

Precoding matrices obtained via SVD\footnote{Singular Value Decomposition} of the MIMO channel matrix can be utilized at the transmitter for optimum power allocation and lower BER\footnote{Bit Error Rate} transmissions. 
However, a key step enabling this improved performance is the feedback of these precoders to the transmitter from the receiver.
Considering the limited bit budget for such CSI\footnote{Channel State Information} feedback, the precoders need to be quantized with single digit number of bits.
For a $N_T \times N_R$\footnote{$N_T(R)$: Number of Transmit (Receive) Antennas} MIMO system, this amounts to quantizing a $N_T \times N_R$ complex valued matrix.
This odious task is helped by the presence of an underlying manifold structure, and temporal/frequency correlations in the precoders. 
In this work, we introduce a reservoir computing framework for the task of prediction of new precoders upon observation of past such precoders, by utilizing temporal correlations. 
This is a departure from existing methods which exploit the non linear geometry endowed by the manifold structure to perform the same.
Alternately, the non-linear relation is captured in our work via the dynamical reservoir state via the training process of the reservoir. 
Simulations reveal reduced quantization error, which results in lower BER as well as improved achievable rate, as compared to previous work.



\end{abstract}



\section{Introduction}
\label{intro}
% Broad summary of your work, come to your problem definition. 
In MIMO wireless systems, precoding matrices are used for matrix transformation of $N_S$-dimensional ($N_S \leq \text{min}(N_T,N_R))$ information vector onto the $N_T$-dimensional transmit vector, corresponding to the signal emanating out of the $N_T$ antennas of the transmitter.
The optimum precoding matrix, to achieve channel capacity via optimum power allocation is obtained via the SVD of the MIMO channel matrix. \cite{love2008overview}
The MIMO channel matrix is estimated at the receiver, and the transmitter has no a-priori knowledge of the same.
Hence, to avail the said benefits of SVD precoders, the receiver needs to quantize and feedback the precoders it obtains from SVD of the MIMO channel matrix.
To respect the bit budget imposed by limited feedback CSI schemes, the quantization needs to be performed only with a few single number of bits.
Given the high dimensionality of the precoders (which form a $N_T \times N_R$ complex valued matrix, viz. $2\times N_T\times N_R$ real numbers), effective quantization with just a few bits pose a significant challenge.

% Elaborate on each point (what people have done before) lecunnas
This challenge has been overcome in past work via two inherent properties of the precoders. 
Firstly, the precoders are not actually $2\times N_T\times N_R$ dimensional entities, and these actually have an underlying manifold structure which allows one to work with lower dimensions if one performs operations over the manifold directly.
Secondly, these precoders are correlated nicely along both time and frequency. 
Capturing temporal correlations allow for predictive quantization algorithms, which allow improvement of the quantization error with time.
Exploiting frequency correlations allow for various interpolation algorithms, which in turn reduce the feedback overhead in a single OFDM frame, by feeding back only for certain subcarriers, and interpolating over the others.
Although the underlying manifold structure allows one to work with effective lower dimensions, most of the naive linear algorithms to predict and interpolate don't work well here, owing to the inherent non-linear structure of the manifold.
Hence, combining these two properties, viz. the underlying manifold structure and temporal-frequency correlations, require generalization of such linear algorithms to the manifold under consideration, by exploiting the differential geometry of the manifold. Relevant past work in this direction has been \cite{Gupt1905:Predictive,6891198,6545375,5671092,Li2016,khaled2005quantized,krishnamachari2013geometry}.

% What is the way ahead
The manifold based approaches have provided significant performance benefits, given that they work with effective lower dimensions.
However, these approaches are difficult to implement and require some familiarity with manifold geometry for a complete grasp.
In this paper, we propose a reservoir computing based predictive quantizer to exploit the temporal correlations effectively.
The proposed scheme offers a cleaner solution to the predictive quantization problem, which brings about both ease of implementability, and improved performance as well, when compared to \cite{6891198}, which proposed a manifold based method of predictive quantization.

% How to we do this 
Reservoir computing is a computational framework designed for sequential data processing. 
It's design takes inspiration from several frameworks of recurrent neural networks \cite{jaeger2004harnessing} including Liquid State Machines \cite{jalalvand2015real}. 
A reservoir computing system maps the inputs to spatio-temporal patterns in higher dimensional dynamical reservoir state. 
The pattern analysis over these spatio-temporal patterns is performed by an output coupler which maps the higher dimensional reservoir state back to the output dimension, generally having lower dimensionality than reservoir state. 
% A important characteristic of reservoir computing is that while training, the input weights and the weights within the reservoir are not kept fixed and the weights of the output coupler are updated. 
The major advantage of reservoir computing system is that itâ€™s extremely easy to train in comparison to its counterparts like Deep Neural Networks. 
The input features can be efficiently read out by a simple learning algorithm because the reservoir non-linearly transforms sequential inputs into the higher dimensional reservoir state. 
Reservoir computing has been successfully used to solve problems like spoken digit recognition \cite{verstraeten2005isolated}, handwritten digit image recognition \cite{jalalvand2015real} and climate prediction \cite{pathak2017using}.  
% Owing to the versatility of reservoir computing, it can deal with any sequential data in principle and hence further expansion of its application fields is widely expected.

Reservoir computing has also been applied to solve problems in the domain of wireless communications \cite{jaeger2004harnessing,mosleh2017brain,shafin2018realizing}. 
\cite{jaeger2004harnessing} used reservoir computing framework to harness channel non-linearities in order for smarter channel equalization.
\cite{mosleh2017brain,shafin2018realizing} utilized the same for OFDM symbol detection while tapping in channel, as well as power amplifier non-linearities.
In our work, we utilize the reservoir computing framework to discover the underlying non-linear relations between previously observed precoding matrices, in order to predict the next precoder in series. Upon vectorizing the precoding matrices, this becomes analogous to time series prediction algorithms, which the reservoir computing framework has been known to work really well for \cite{mosleh2017brain}.

% 

\section{System Model}
\label{section2}
\noindent We consider a point-to-point MIMO-OFDM wireless system that
has $N_T$ transmit antennas and $N_R$ receive antennas. 
The available bandwidth is divided into $N$ subcarriers, so that individually for each subcarrier, the channel can be considered flat fading. 
The transmitter communicates an $N_s \times 1$ information data vector, where
$N_s \leq \text{min}(N_T,N_R)$, and the $N_T \times N_s$ precoding
matrix maps the $N_s \times 1$ information data vector onto the $N_T \times 1$ transmit vector emanating out of the transmitter.

In this discussion, we assume that $N_T > N_R$ and $N_s = N_R$. Keeping notations consistent with \cite{6891198,Gupt1905:Predictive}, the $N_R \times 1$ data stream received is denoted by:
\begin{equation}
\by_{i,t}=\bH^{H}_{i,t}\tilde{\bU}_{i,t}\bx_{i,t}+ \bw_{i,t}
\end{equation}
Here $\tilde{\bU}_{i,t} \in \mathbb{C}^{N_T \times N_R}$ denotes the precoding matrix which is a function of quantized CSI fed back by the receiver, $\by_{i,t} \in \mathbb{C}^{N_R \times 1}$ is the received data stream, $\bx_{i,t} \in \mathbb{C}^{N_R \times 1}$ denotes the transmitted signal, all at the $i$-th subcarrier of the $t$-th OFDM frame, $\bH_{i,t} \in \mathbb{C}^{N_T \times N_R}$ denotes the MIMO channel matrix and $\bw_{i,t}$ denotes the i.i.d. complex Gaussian noise with $\bw_{i,t} \sim \mathcal{N}_{\mathbb{C}}(0,N_0\bI_{N_R})$, $N_0$ being the noise variance.

% Tell that if infinite bit budget available, transmitter can directly use matrices obtained from SVD as precoders. However, since this is not the case we have to quantize (N dimensional) manifold, incurring substantial quantization error. Independent quantization techniques (references). Predictive quantization techniques can be used on the top of independently initialized values to improve the error. This is done in a way intuitively similar to Delta PCM, in which quantization error is reduced substanitally by using a linear predictor and quantizing the difference between the predicted and observed value. Since these preocders lie on Stiefel manifold, linear schemes will not work, due to non linearity of the manifold. However,past work has generalized the ideas to the manifold by exploiting the differential geometry of the manifold. Once a prediction is obtained, one has to quantize only the difference between the prediction and the observed value, which allows for improved quantization error. As the prediction becomes closer to the original value, the quantization error also decreases. The quantization algorithm is illustrated in Section 3.3   


Provided an infinite bit budget for quantization, the transmitter can directly use the matrices obtained from SVD of $\bH^{H}_{i,t}$ as precoder $\tilde{\bU}_{i,t}$.
In particular, if $\text{SVD}(\bH_{i,t}) = \bU_{i,t},\Sigma_{i,t}, \bV_{i,t}$, then $\tilde{\bU}_{i,t} = \bU_{i,t}$.
Notice that matrices $\bU_{i,t}$ reside on the Stiefel manifold $\text{St}(N_T,N_R)$, since the columns of $\bU_{i,t}$ form a set of $N_R$ orthogonal vectors in $N_T$ dimensions \cite{Gupt1905:Predictive,6891198}.  Given practical limitations, only limited feedback is available from the receiver, and the objective is to find a `reasonable' estimate of $\bU_{i,t}$, given by $\tilde{\bU}_{i,t}$ using the available feedback bits.

Algorithms similar to Lloyd codebook algorithm have been studied for quantizing $\text{St}(N_T,N_R)$ \cite{6678348}.
However, this method treats precoders close-by in time/frequency to each other as independent, since it doesn't exploit any temporal/spectral correlations for quantization. 
Predictive quantization algorithms \cite{Gupt1905:Predictive,6891198} can be utilized to reduce the quantization error with time by tapping in available correlations.

We now discuss the basic mathematical model central to predictive quantization algorithms for precoding matrices.
Consider that we predict based on past $p$ observed precoding matrices, and hence $\tilde{\bU}_{i,t-p},\tilde{\bU}_{i,t-p+1},\ldots,\tilde{\bU}_{i,t-2},\tilde{\bU}_{i,t-1}$ have been fed back and available at the transmitter.
Using a prediction algorithm and exploiting time correlations among precoders at $t-p,t-p+1,\ldots,t-2,t-1$ time instances, the transmitter can predict $\tilde{\bU}_{i,t}$ to be some $\bP_{i,t}=f_P(\tilde{\bU}_{i,t-p},\tilde{\bU}_{i,t-p+1},\ldots,\tilde{\bU}_{i,t-2},\tilde{\bU}_{i,t-1})$, where $f_P: \text{St}(N_T,N_R)^p \to \text{St}(N_T,N_R)$ is a prediction function.
Now since the transmitter already knows a coarse estimate of $\tilde{\bU}_{i,t}$ given by $\bP_{i,t}$, the receiver just quantizes the local space of $\text{St}(N_T,N_R)$ nearby $\bP_{i,t}$ to enable a refined estimate of $\tilde{\bU}_{i,t}$ at the transmitter.
That is, the transmitter utilizes a quantization function $q_P:\text{St}(N_T,N_R) \times \mathbb{N} \to \text{St}(N_T,N_R)$, to obtain $\tilde{\bU}_{i,t} \gets q_P(\bP_{i,t},\text{fb}_{i,t})$, where $\text{fb}_{i,t} \in \{1,2\ldots2^{\mathcal{B}}\}$ is the feedback from the receiver for precoder at $i$-th subcarrier and $t$-th time instant, considering bit budget of $\mathcal{B}$ bits. 
In \cite{Gupt1905:Predictive,6891198,6545375}, $\text{fb}_{i,t}$ denotes the codeword index for the codebook obtained for quantizing the local tangent space at $\bP_{i,t}$.

Since using predictive quantization scheme, quantization is on a smaller subspace as compared to the entire manifold in case of independent quantization, the quantization error is substantially lower as well.
However, that being said, independent quantization algorithms are important for initialization the predictive quantization strategies.
Observe that, predictive quantization algorithms for SVD precoders are typically the higher dimensional analogs of the well known approaches to reduce quantization error using Linear Prediction Codes and Delta PCM.  
In both LPC and Delta PCM, quantization error is reduced substantially by using a linear predictor and quantizing the scalar difference between the predicted and observed value. 
Analogous to this, for $\text{St}(N_T,N_R)$ quantization error is reduced by quantizing a smaller subspace around predicted value, compared to the entire manifold.

In our work, we propose a reservoir computing based prediction framework, which predicts the new precoders by tapping the temporal correlations captured from past observed precoders ($f_P$). 
We reuse the $q_P$ algorithm from \cite{Gupt1905:Predictive,6891198}, however, by incorporating the $q_P$ in the prediction framework, as discussed in subsequent sections, the $f_P$ function also exploits correlations in $q_P$ (codebook locality correlations), along with temporal correlations. 
Previous work \cite{Gupt1905:Predictive,6891198} have $f_P$ and $q_P$ independent of each other.
The detailed discussion on reservoir computing based model for $f_P$ and $q_P$ is presented in Section \ref{section3}. 
The proposed reservoir computing scheme is evaluated against the temporal predictive quantization scheme in \cite{6891198}, and the simulation results are discussed in Section \ref{section4}. Finally, Section \ref{section5} concludes and presents future lines of work. 

%Explain prediction and quantization steps here itself ! (succintly)
\section{Reservoir model for Predictive Quantization}
\label{section3}
\subsection{Vectorizing matrices in Stiefel Manifold}
The reservoir computing framework has inputs/outputs as vectors, and hence it is necessary to convert the precoder matrices to vectors. 
A precoding matrix $\mathcal{\bM} \in \text{St}(N_T,N_R)$ is a $N_T\times N_R$ matrix with the property $\mathcal{\bM}^H\mathcal{\bM}=\mathbb{I}_{N_R}$. 
For brevity sake, let us say that such a matrix $\mathcal{\bM}$ is a \'semi-unitary\' matrix.
A naive method of vectorizing a $N_T\times N_R$ semi-unitary matrix would be to take all it's $N_TN_R$ complex numbers individually and stack them onto a $2N_TN_R$ dimensional vector.
However, we can do this a little more efficiently, considering the $\mathcal{\bM}^H\mathcal{\bM}=\mathbb{I}_{N_R}$ property, which implies that each vector representing the columns of $\mathcal{\bM}$ is orthogonal to other columns.
For the first column, we take the $N_T$ complex numbers and stack them onto a $2N_T$ vector. 
For the second column, we take only the first $N_T-1$ complex numbers, since the last number can be determined by utilizing orthogonality with the first vector. 
For the third column, we take only the first $N_T-2$ complex numbers, and so on. 
Hence, we can vectorize the $N_T\times N_R$ $\mathcal{\bM}$ into a $2(N_T)+2(N_T-1)+\ldots+2(N_T-N_R+1)= 2N_TN_R-(N_R^2-N_R)$ dimensional vector. 
For any semi-unitary matrix $\mathcal{\bM}$, denote by $\mathcal{\bm}$ the vectorized representation.

Observe here that we do pay a dimensional penalty upon vectorizing the semi-unitary $\mathcal{\bM}$. 
$\mathcal{\bM} \in \text{St}(N_T,N_R)$, which is a $2N_TN_R-N_R^2$ dimensional manifold. 
We vectorize $\mathcal{\bM}$ to a $2N_TN_R-N_R^2+N_R$ dimensional vector, an overhead of $N_R$ dimensions, since we didn't exploit the fact that each column vector is a norm 1 vector as well.
It looks as if we could just take $N_T-1$ complex numbers when vectorizing the first column, instead of $N_T$, but this would lead to a non-unique representation, since there could then be infinite possibilities of the missing one complex number, such that the vector has a norm 1.
%Focus on the fact that you don't actually pay a penalty
%The thing to quantize remains on the Stiefel Manifold
\subsection{Prediction and Quantization}
The vector representation of semi-unitary matrices allow us to proceed with the discussion on reservoir computing framework for predictive quantization of SVD precoders. Consider the reservoir computing framework as in Fig. %Ref Fig. Reservoir Pic

The input to the reservoir $i$ corresponding to subcarrier $i$, is $\tilde{\bu}_{i,t-1}$, which is the vectorized representation of $\tilde{\bU}_{i,t-1}$. 
Let $D_{\text{in}}=2N_TN_R-N_R^2+N_R$ be the dimension of the input vector to the dimension.
Using a randomly initialized $D_{\text{in}}\times D_{\text{resv}}$ input coupler matrix $\bW^{\text{in}}_i$, the $D_{\text{in}}$ dimensional input is mapped to a $D_{\text{resv}}$ dimensional vector, where $D_{\text{resv}}$ is the dimension of the reservoir state vector $\br_{i,t}$. 
The reservoir state vector $\br_{i,t}$ is initialized with $D_{\text{resv}}$ zeros, and is updated via the following equation,
\begin{equation}
\br_{i,t}=\text{tanh}(\bA_i\br_{i,t-1}+\bW^{\text{in}}_i\tilde{\bu}_{i,t-1})
\label{res_upd}
\end{equation}
where $\bA_i$ is the adjacency matrix which captures the reservoir dynamics and correlates reservoir state across time, and tanh is applied element wise. 
Typical choices for $\bA_i$ have been erdos-renyi graphs with an upper bounded maximum eigen value \cite{mosleh2017brain,pathak2017using}.

The output of the reservoir is $\bp_{i,t}$, a vectorized representation of $\bP_{i,t}$. 
Hence, the output of the reservoir is also a $D_{\text{in}}$ dimensional vector ($D_{\text{out}}=D_{\text{in}}$). 
The output $\bp_{i,t}$ is obtained via matrix transformation of the reservoir state $\br_{i,t}$ via the $D_{\text{resv}}\times D_{\text{out}}$ output coupler matrix $\bW^{\text{out}}_i$, viz.
\begin{equation}
\bp_{i,t}=\bW^{\text{out}}_i\br_{i,t}
\label{pred_op}
\end{equation}

This completes the forward prediction algorithm to obtain $\bP_{i,t}$ using just one past observed $\bU_{i,t-1}$. That is,
\begin{equation}
\bP_{i,t}=f_P(\tilde{\bU}_{i,t-1})=\bW^{\text{out}}_i\text{tanh}(\bA_i\br_{i,t-1}+\bW^{\text{in}}_i\tilde{\bu}_{i,t-1})\footnote{For Brevity sake, $\bp_{i,t}$ and $\bP_{i,t}$ are treated to be one and the same}
\label{fwd_pred}
\end{equation}

However, a complete discussion of the prediction algorithm also involves a backward pass, involving the updation of output couplers $\bW^{\text{out}}_i$, which would follow next.

Before proceeding ahead, it is critical to detail out the quantization function $q_P(\bP_{i,t},\text{fb}_{i,t})$. 
The quantization method is similar to the one used in \cite{6891198,Gupt1905:Predictive,6545375}. 
Central to the quantization algorithm is the fact that tangent spaces local to a point in manifold, are actually linear vector spaces.
Given two points $\bX$ and $\bY$ in $\text{St}(N_T,NR)$, a lifting operation $\bT^Y_X=\texttt{lift}(\bX,\bY)$, $\texttt{lift}:\text{St}(N_T,NR)^2 \to \mathcal{T}_X \text{St}(N_T,NR)$ gives a tangent from X to Y, $\bT^Y_X \in \mathcal{T}_X \text{St}(N_T,NR)$, where $\mathcal{T}_X \text{St}(N_T,NR)$ is the local tangent space at $\bX$.
A corresponding retraction operation $Y=\texttt{retract}(X,\bT^Y_X)$gives back the manifold point obtained by traversing in the tangent direction.
In this work, the chosen lifting-retraction operations are the Cayley Exponentials, elaborated in \cite{DBLP:journals/corr/abs-1708-00045} and used in \cite{Gupt1905:Predictive}.
The Cayley exponential lifting operation maps two points in $\text{St}(N_T,N_R)$, to a $N_T\times N_T$ skew hermitian matrix representing the tangent from first point to the other. 

The quantization algorithm exploits the linear vector space property of the tangent space and quantizes the local tangent space at the predicted precoder $\bP_{i,t}$.
The codebook for the local tangent space basically corresponds to a collection of codewords representing the different directions (tangents) in the local tangent space.
This codebook is also referred to as the base codebook for the quantization function.
The feedback from the receiver indicates the optimum direction in the base codebook, which the transmitter can choose to get closest to the actual value $\bU_{i,t}$.
With the optimum direction chosen, the next step is to determine how much to move in that particular direction, viz. the length of the chosen tangent direction.
For this, we adopt the strategy in \cite{6891198}, which
controls the magnitude of tangent steps, by having two codebooks
$T^{\mathcal{C}}_p,T^{\mathcal{C}}_m$ of different spreads, but same
$2^{\mathcal{B}-1}$ base vectors from the base codebook. 
The base vectors correspond to $2^{\mathcal{B}-1}$ quantized directions, and by spread of codewords, we mean the length of base vectors considered, with $T^{\mathcal{C}}_p$ having tangents of higher length than $T^{\mathcal{C}}_m$. 
The two codebooks are concatenated to form a $2^\mathcal{B}$ length codebook $T^\mathcal{C}$. 
The receiver finds the optimal index $c_n$ in $T^\mathcal{C}$ by comparing the chordal distance metric ($d_s$) of Stiefel Manifold \cite{Gupt1905:Predictive,6891198}, of each codeword to the actual precoder $\bU_{i,t}$ obtained from the SVD of the channel matrix, using \eqref{eqn:cn1}. The receiver then feeds back $c_n$ to the transmitter using $\mathcal{B}$ bits. The transmitter uses the fed back $c_n$ and \eqref{eqn:cn2} to calculate $\tilde{\bU}_{i,t}$,
\begin{equation}
\label{eqn:cn1}
c_n \gets \text{argmin}_{i \in 2^\mathcal{B}}\left (d_s\left (\bU_{k,l},\text{retract}(\bP_{k,l},T^\mathcal{C}[i])\right ) \right)
\end{equation}
\begin{equation}
\tilde{\bU}_{k,l}=\text{retract}(\bP_{k,l},T^\mathcal{C}[c_n])
\label{eqn:cn2}
\end{equation}


% \textcolor{blue}
{We now describe the algorithm used to control the spread of the codebooks, as discussed in \cite{6891198}. The base codebook ($\mathcal{T}^B$) consists of
  $2^{\mathcal{B}-1}$ matrices belonging to the local tangent space at
  $\bP_{k,l}$, which represent the $2^{\mathcal{B}-1}$ quantized directions. The two codebooks,
  $T^{\mathcal{C}}_p,T^{\mathcal{C}}_m$, have the same set of base
  vectors, but different spreads $g^{s_p}$ and $g^{s_m}$, where $g$ is
  the growth factor and $s_p,s_m$ control the spread of the two
  codebooks. Depending on whether $c_n \geq 2^{\mathcal{B}-1}$,
  i.e. whether the optimum codeword is in $T^{\mathcal{C}}_p$ or
  $T^{\mathcal{C}}_m$, the scale parameter $s[k]$, which in turn
  controls values of $s_p, s_m$ is updated in the following manner,
$$s_p=g^{\text{min}(s[k-1]+1,0)}, s_m = g^{s[k-1]-1}$$
$$
s[k]=
\begin{cases}
\text{min}(s[k-1]+1,0), \text{ for }c_n \in T^{\mathcal{C}}_p\\
s[k-1]-1, \text{otherwise}\\
\end{cases}
$$
with $s[0]=0$. Intuitively, the algorithm reduces/increases the spread
of the codebook till the operation of reduction/increase is no longer
beneficial, i.e. the optimum codeword lies in the higher/lower spread
codebook instead. The scheme we use for obtaining the quantized
directions, however, differs slightly from the one presented in
\cite{6891198}. As discussed in Section \ref{section3}, the Cayley
type lifting map takes inputs from the Stiefel manifold and maps them
to $N_T \times N_T$ skew Hermitian matrices. This allows for vector
quantization of the tangent space, since a skew Hermitian matrix can
be easily converted to a vector, by taking the upper triangular
entries and stacking them in a vector. Vector quantization for the
base codebook, $\mathcal{T}^B$ allows for isotropic $2^{\mathcal{B}-1}$ quantized
directions for the tangent space local to $\bP_{k,l}$. \cite{6891198}
proposed a randomly initialized $\mathcal{T}^B$ which does not guarantee
isotropic $2^{\mathcal{B}-1}$ quantized directions, which can
potentially hurt the performance of the system in the cases when
random initialization gives directions which are close to each
other. More details on codebook construction are provided in Section
\ref{section5}. This concludes the proposed predictive quantization
algorithm.}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% More information needed about the adjacency
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Simulation Results}
\label{section4}
%Qtisn error with 1000 chan evols
%Qtisn error with 100 chan evols
%BER
%Achievable rat
\section{Conclusions and Future Work}
\label{section5}


% \section{Acknowledgment}
% % \label {section6}
% % \input{sections/6_section.tex}
% Parts of this work was supported by the Bharti Centre for Communication in
% IIT Bombay, and the Visvesvaraya
% PhD Scheme of Ministry of Electronics \& Information Technology,
% Government of India, being implemented by Digital India Corporation.


\renewcommand{\bibfont}{\footnotesize}
\bibliography{IEEEabrv,main}
\bibliographystyle{IEEEtran}
\end{document}

% Remove the first para of FSM. State what r1,r2,d,\theta are in the FSM model states themselves. As a concluding statement of STAT-2 state that using the 
% inherit geometery of the problem, we get AoA
