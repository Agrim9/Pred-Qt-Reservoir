%% This is a skeleton file demonstrating the use of IEEEtran.cls (requires IEEEtran.cls version 1.8a or later) with an IEEE conference paper.
%%
%% Modified by Khan Reaz( kahn.reaz@ieee.org)
%% Support sites:
%% http://www.ieee.org/

%%***********************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or implied; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE!
%% User assumes all risk and can modify as s/he wants.

%%***********************************************************

\def\b0{{\bf 0}}
\def\ba{{\bf a}}
\def\bb{{\bf b}}
\def\bc{{\bf c}}
\def\bd{{\bf d}}
\def\be{{\bf e}}
\def\bg{{\bf g}}
\def\bh{{\bf h}}
\def\bi{{\bf i}}
\def\bj{{\bf j}}
\def\bk{{\bf k}}
\def\bl{{\bf l}}
\def\bm{{\bf m}}
\def\bn{{\bf n}}
\def\bo{{\bf o}}
\def\bp{{\bf p}}
\def\bq{{\bf q}}
\def\br{{\bf r}}
\def\bs{{\bf s}}
\def\bt{{\bf t}}
\def\bu{{\bf u}}
\def\bv{{\bf v}}
\def\bw{{\bf w}}
\def\bx{{\bf x}}
\def\by{{\bf y}}
\def\bz{{\bf z}}


\def\bSigma{{\bf \Sigma}}
\def\bA{{\bf A}}
\def\bB{{\bf B}}
\def\bC{{\bf C}}
\def\bD{{\bf D}}
\def\bE{{\bf E}}
\def\bF{{\bf F}}
\def\bG{{\bf G}}
\def\bH{{\bf H}}
\def\bI{{\bf I}}
\def\bJ{{\bf J}}
\def\bK{{\bf K}}
\def\bL{{\bf L}}
\def\bM{{\bf M}}
\def\bN{{\bf N}}
\def\bO{{\bf O}}
\def\bP{{\bf P}}
\def\bQ{{\bf Q}}
\def\bR{{\bf R}}
\def\bS{{\bf S}}
\def\bT{{\bf T}}
\def\bU{{\bf U}}
\def\bV{{\bf V}}
\def\bW{{\bf W}}
\def\bX{{\bf X}}
\def\bY{{\bf Y}}
\def\bZ{{\bf Z}}


%package list
\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
\let\labelindent\relax
\usepackage{enumitem}
\usepackage{fleqn}
\usepackage{cite}
\usepackage{graphicx}
\usepackage[varg]{newtxmath}
\graphicspath{ {images/} }
\usepackage{pdfpages}
\usepackage{wrapfig}
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage{lettrine}
\usepackage{amsmath}
% \usepackage{titlesec}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{float}
\usepackage[font={footnotesize}]{caption}
\usepackage[numbers,sort,square,compress]{natbib}
\usepackage[para]{footmisc}
\usepackage{xcolor}
\setlength{\belowcaptionskip}{-12pt}
\newcommand{\highlight}[1]{%
  \colorbox{red!50}{$\displaystyle#1$}}

% \usepackage{parskip}
% \setlength{\parskip}{0.02\baselineskip}

% \fancypagestyle{plain}{
%   \fancyhf{} % sets both header and footer to nothing
% \renewcommand{\headrulewidth}{0pt}
%   \fancyhead[C]{2018 International Conference on Indoor Positioning and Indoor Navigation (IPIN), 24-27 September 2018, Nantes, France}% Right header

% }
\pagestyle{plain}% Set page style to plain.

% \pagestyle{fancyplain}
% \fancyhf{}
% \renewcommand{\headrulewidth}{0pt}
% \fancyhead[C]{2018 International Conference on Indoor Positioning and Indoor Navigation (IPIN), 24-27 September 2018, Nantes, France}
\DeclareMathOperator*{\argmin}{argmin}
% \usepackage{titlesec}

% \titlespacing*{\section}{0pt}{1pt plus 2p}{1ex}
% \titleformat*{\section}{\fontsize{12}{12}\bfseries}
% \titleformat{\section}
%        {\normalfont\fontfamily{phv}\fontsize{12}{17}\bfseries}{\thesection}{1em}{}

% \titleformat{\subsection}
%        {\normalfont\fontfamily{phv}\fontsize{12}{17}\bfseries\itshape}{\thesubsection}{1em}{}

\begin{document}
%Here goes the title

\title{Predictive Quantization for MIMO-OFDM SVD Precoders using Reservoir Computing Framework}


%Authors List

 \author{\authorblockN{Agrim Gupta, Pranav Sankhe, Kumar Appaiah, Manoj Gopalkrishnan}
 \
 \authorblockA{Department of Electrical Engineering, Indian Institute of Technology Bombay\\
 \{agrim,pranavs,akumar,manojg\}@ee.iitb.ac.in}
% \thanks{Parts of this work was supported by the Bharti Centre for Communication in
% IIT Bombay, and the Visvesvaraya
% PhD Scheme of Ministry of Electronics \& Information Technology,
% Government of India (implemented by the Digital India Corporation).
% }}
}
\maketitle

\thispagestyle{plain}
%Main body starts



  % \noindent We consider problem of quantization and interpolation of
  % time and frequency varying precoding matrices in wireless MIMO
  % systems. Knowledge of precoder matrices at the transmitter can We first establish a result comparing the performance of
  % quantization and interpolation attempted directly over the Stiefel
  % manifold rather than over the Unitary manifold, as has been the
  % primary approach thus far. We propose a predictive quantization
  % algorithm to improve upon the quantization metric by exploiting both
  % time and frequency correlations in a constructed frequency hopping
  % scenario. Building upon these, we finally propose a joint
  % time-frequency interpolating algorithm to estimate the precoders
  % which were not fed back. We demonstrate significant improvements in
  % both BER curve and log-sum rate capacity over the existing
  % approaches.
\begin{abstract}

  % Precoding transmissions in wireless MIMO systems is essential to
  % enable optimal utilization of the spatial degrees of
  % freedom. However, communicating the precoding matrices from the
  % receiver is challenging, owing to large feedback requirements. Past
  % work has shown that predictive quantization in time, as well as
  % interpolation over frequency can be used to reconstruct the
  % precoders over a wide band, although these techniques have not been
  % used jointly. We propose both a predictive quantization as well as a
  % joint time-frequency interpolation strategy for precoding matrices
  % over the Stiefel manifold. The key insight that we use is that
  % local tangent spaces in the manifold permit effective combination of
  % both temporal and frequency domain information for more accurate
  % precoder reconstruction. Simulations reveal that we obtain a
  % significant improvement in achievable rate as well as BER reduction
  % when compared to existing strategies.


Precoding matrices obtained from SVD of the MIMO channel matrix (which is estimated at the receiver) can be utilized at the transmitter for optimum power allocation and lower BER transmissions. 
A key step enabling this improved performance is the feedback of these precoders to the transmitter from the receiver.
Since the bit budget for such channel state information (CSI) feedback is limited, these precoders need to be quantized effectively using just a few bits.
For a $N_T \times N_R$ MIMO system ($N_{\{T/R\}}$: Number of Transmit/Receive Antennas), this amounts to quantizing a $N_T \times N_R$ complex-valued matrix.
This task is aided by the presence of an underlying manifold structure and temporal/frequency correlations in the precoders. 
Predictive quantization methods reduce the quantization error by predicting new precoders based on observation of past precoders, and then quantizing the information required to obtain the actual precoder from the predicted value. 
In this work, we introduce a reservoir computing based framework for predictive quantization by exploiting temporal correlations. 
Past methods have primarily exploited the nonlinear geometry endowed by the manifold structure for precoder prediction.
Here, this non-linear relationship is captured using the dynamical reservoir state as part of the online training process of the reservoir.
Simulations reveal that this approach produces reduced quantization error, thereby resulting in lower BER as well as improved achievable rates when compared to earlier work.
\end{abstract}



\section{\textbf{Introduction}}
\label{intro}
% Broad summary of your work, come to your problem definition.
In MIMO wireless systems, precoding matrices are used for transformation of an $N_S$-dimensional ($N_S \leq \text{min}(N_T,N_R))$ information vector onto an $N_T$-dimensional transmit vector that corresponds to the signal emanating from the $N_T$ antennas of the transmitter.
The precoding matrix used to achieve the channel capacity using optimal power allocation is obtained via the SVD of the MIMO channel matrix \cite{love2008overview}.
The MIMO channel matrix is estimated at the receiver, and the transmitter has no a priori knowledge of the same.
Therefore, to obtain the said benefits of SVD precoders at the transmitter, the receiver needs to quantize and feed back the precoders it obtains from the SVD of MIMO channel matrix.
To respect the bit budget imposed by limited feedback CSI schemes, the quantization needs to be performed using very few bits.
Given the high dimensionality of precoders (which form a $N_T \times N_R$ complex-valued matrix, viz. $2N_TN_R$ real numbers), effective quantization with just a few bits poses a significant challenge.

% Elaborate on each point (what people have done before) lecunnas
Past work has used two inherent properties of the precoders to enable effective quantization with just a few bits.
First, the precoders are not actually $2N_TN_R$ dimensional entities, but have an underlying manifold structure that allows to work with lower dimensions, when performing operations over the manifold directly.
Second, these precoders are correlated in both time and frequency.
Utilizing these correlations permit the use of predictive quantization algorithms, which enable improvement of the quantization error with time.
This improvement is obtained because utilizing the available correlations allow for quantization of just the small extra information needed on top of the past fed back estimates, for estimation of the new precoder, as compared to quantization of the complete new precoder information in the entirety.
Exploiting frequency correlations permits the use of interpolation algorithms, which reduce the feedback overhead in a single OFDM frame, by feeding back CSI only for certain subcarriers, and interpolating over the others.
Combining the two inherent properties (viz. manifold structure and correlations), requires a generalization of various well known linear algorithms for prediction, quantization and interpolation, to the manifold structure under consideration. This is done by exploiting the underlying non-linear differential geometry of the manifold. Relevant past work in this direction has been presented in \cite{Gupt1905:Predictive,6891198,6545375,5671092,Li2016,khaled2005quantized,krishnamachari2013geometry,chang2011adaptive}.

% What is the way ahead
Manifold based approaches for predictive quantization have provided significant performance benefits, given that they work with the effective lower dimensions while framing the manifold operations to predict and quantize.
However, these approaches are non-trivial, since they require specific operations over the non-linear manifold differential geometry.
In this paper, we propose a reservoir computing based predictive quantizer to exploit the temporal correlations effectively.
The proposed scheme offers a simple solution to the predictive quantization problem, which brings about ease of implementation, and improved performance as well, when compared to \cite{6891198}, which proposed a manifold based method of predictive quantization by exploiting temporal correlations.

% How to we do this
%\textcolor{blue}
{
Reservoir computing is a computational framework designed for sequential data processing.
Its design is inspired from several frameworks of recurrent neural networks \cite{jaeger2004harnessing}, including Liquid State Machines \cite{jalalvand2015real} and Echo State Networks \cite{mosleh2017brain,pathak2017using}.
The reservoir computing framework represents the non-linear relationships in the data via the higher dimensional dynamical reservoir state vector, which is transformed (using
matrices, referred to as `couplers') to the lower dimensional input/output vectors
% A reservoir computing system maps the inputs to temporal patterns in a higher dimensional dynamical reservoir state.
% The pattern analysis over these temporal patterns is performed by an output coupler that maps the higher dimensional reservoir state back to the output dimension that generally possesses lower dimensionality than the reservoir state.
% A important characteristic of reservoir computing is that while training, the input weights and the weights within the reservoir are not kept fixed and the weights of the output coupler are updated.
The key advantage of a reservoir computing framework is that it is easy to train in comparison to its counterparts like Deep Neural Networks \cite{mosleh2017brain}.
Reservoir computing has been successfully used to solve problems like handwritten digit image recognition \cite{jalalvand2015real}, climate prediction \cite{pathak2017using} and spoken digit recognition \cite{verstraeten2005isolated}.
}
% Owing to the versatility of reservoir computing, it can deal with any sequential data in principle and hence further expansion of its application fields is widely expected.
Reservoir computing frameworks have also been applied to solve problems in the domain of wireless communications \cite{jaeger2004harnessing,mosleh2017brain,shafin2018realizing}.
\cite{jaeger2004harnessing} used reservoir computing framework to harness channel non-linearities for improved channel equalization.
\cite{mosleh2017brain,shafin2018realizing} utilized the framework for OFDM symbol detection while accounting for channel and power amplifier non-linearities.
In our work, we utilize the reservoir computing framework to capture the underlying nonlinear relations between previously observed precoders, in order to predict the precoder at next time instant. This becomes analogous to time series prediction algorithms, for which the reservoir computing framework has been cited to work effectively \cite{mosleh2017brain}.

\section{\textbf{System Model}}
\label{section2}
We consider a point-to-point $N_T\times N_R$ MIMO-OFDM wireless system. 
In this discussion, we assume that $N_T > N_R$ and $N_s = N_R$.
The available bandwidth is divided into $N$ subcarriers, so that, individually for each subcarrier, the channel can be assumed to be flat fading.
Consistent with notation in \cite{6891198,Gupt1905:Predictive}, the data stream received on the $i$-th subcarrier, $t$-th time instant (or the $t$-th OFDM frame) is denoted by:
\begin{equation}
\by_{i,t}=\bH^{H}_{i,t}\tilde{\bU}_{i,t}\bx_{i,t}+ \bw_{i,t}
\end{equation}
Here, $\tilde{\bU}_{i,t} \in \mathbb{C}^{N_T \times N_R}$ is the quantized estimate of precoding matrix, $\by_{i,t} \in \mathbb{C}^{N_R \times 1}$ is the received data stream, $\bx_{i,t} \in \mathbb{C}^{N_T \times 1}$ denotes the transmitted signal, $\bH_{i,t} \in \mathbb{C}^{N_T \times N_R}$ denotes the MIMO channel matrix and $\bw_{i,t}$ denotes the i.i.d. complex Gaussian noise with $\bw_{i,t} \sim \mathcal{N}_{\mathbb{C}}(0,N_0\bI_{N_R})$, $N_0$ being the noise variance. For this work, we assume that $\bH^{H}_{i,t}$ is estimated exactly, with zero error at the receiver.

% Tell that if infinite bit budget available, transmitter can directly use matrices obtained from SVD as precoders. However, since this is not the case we have to quantize (N dimensional) manifold, incurring substantial quantization error. Independent quantization techniques (references). Predictive quantization techniques can be used on the top of independently initialized values to improve the error. This is done in a way intuitively similar to Delta PCM, in which quantization error is reduced substanitally by using a linear predictor and quantizing the difference between the predicted and observed value. Since these preocders lie on Stiefel manifold, linear schemes will not work, due to non linearity of the manifold. However,past work has generalized the ideas to the manifold by exploiting the differential geometry of the manifold. Once a prediction is obtained, one has to quantize only the difference between the prediction and the observed value, which allows for improved quantization error. As the prediction becomes closer to the original value, the quantization error also decreases. The quantization algorithm is illustrated in Section 3.3


If an infinite bit budget is available for precoder quantization, the transmitter can directly use the matrices obtained from SVD of $\bH^{H}_{i,t}$ as the precoder $\tilde{\bU}_{i,t}$. 
That is, if $\text{SVD}(\bH_{i,t}) = \bU_{i,t},\Sigma_{i,t}, \bV_{i,t}$, then $\tilde{\bU}_{i,t} = \bU_{i,t}$.
Notice that matrices $\bU_{i,t}$ reside on the Stiefel manifold $\text{St}(N_T,N_R)$, since the columns of $\bU_{i,t}$ form a set of $N_R$ orthogonal vectors in $N_T$ dimensions \cite{Gupt1905:Predictive,6891198}.
Given practical limitations, only limited feedback is available from the receiver, and the objective is to estimate $\bU_{i,t}$, via $\tilde{\bU}_{i,t}$, using the available feedback bits. Independent quantization algorithms analogous to the Lloyd codebook algorithm have been studied for quantizing $\text{St}(N_T,N_R)$ \cite{6678348}.
However, this approach treats precoders that are close to each other in time/frequency as independent, since it does not exploit any temporal/spectral correlations for quantization.

Predictive quantization algorithms \cite{Gupt1905:Predictive,6891198} can be utilized to reduce the quantization error by utilizing the available correlations.
We now discuss the basic mathematical model that is central to predictive quantization algorithms for precoding matrices.
Suppose that we predict the current precoding matrix based on the past $p$ observed precoding matrices. For this, assume that $\tilde{\bU}_{i,t-p},\tilde{\bU}_{i,t-p+1},\ldots,\tilde{\bU}_{i,t-2},\tilde{\bU}_{i,t-1}$ have been fed back and available at the transmitter.
By using a prediction algorithm and by exploiting time correlations among precoders at time instances $t-p,t-p+1,\ldots,t-2,t-1$, the transmitter can obtain a prediction (viz. a coarse estimate) of $\bU_{i,t}$, given by $\bP_{i,t}=f_P(\tilde{\bU}_{i,t-p},\tilde{\bU}_{i,t-p+1},\ldots,\tilde{\bU}_{i,t-2},\tilde{\bU}_{i,t-1})$, where $f_P: \text{St}(N_T,N_R)\times\ldots\{p-\text{times}\}\ldots\times\text{St}(N_T,N_R) \to \text{St}(N_T,N_R)$ is a prediction function.
Now, since the transmitter already possesses a coarse estimate $\bP_{i,t}$ of $\bU_{i,t}$, the receiver just quantizes the local space of $\text{St}(N_T,N_R)$ nearby $\bP_{i,t}$, instead of the complete $\text{St}(N_T,N_R)$. The receiver feeds back this quantized information, enabling a refined estimate of $\bU_{i,t}$, given by $\tilde{\bU}_{i,t}$ at the transmitter.
That is, the transmitter utilizes a quantization function $q_P:\text{St}(N_T,N_R) \times \mathbb{N} \to \text{St}(N_T,N_R)$, to obtain $\tilde{\bU}_{i,t} \gets q_P(\bP_{i,t},\text{fb}_{i,t})$, where $\text{fb}_{i,t} \in \{1,2\ldots2^{\mathcal{B}}\}$ is the feedback from the receiver for precoder at $i$-th subcarrier and $t$-th time instant, considering bit budget of $\mathcal{B}$ bits.
In \cite{Gupt1905:Predictive,6891198,6545375}, $\text{fb}_{i,t}$ denotes the codeword index for the codebook obtained for quantizing the local tangent space at $\bP_{i,t}$.

When using the predictive quantization scheme, the quantization is performed on a smaller subspace when compared to the entire manifold in case of independent quantization.
Therefore, the quantization error is substantially lower as well.
However, independent quantization algorithms are important for smart initialization (instead of a cold start) of the predictive quantization strategies.
Observe that predictive quantization algorithms for SVD precoders are typically the higher dimensional analogues of well known approaches to reduce quantization error using linear prediction codes (LPCs) and Delta PCM.
In both LPCs and Delta PCM, quantization error can be reduced substantially using a linear predictor and quantizing the scalar difference between the predicted and observed value (innovation).
Analogous to this, for $\text{St}(N_T,N_R)$, quantization error is reduced by quantizing a subspace around the predicted value that is much smaller than the entire manifold.

In our work, we propose a reservoir computing based prediction framework that predicts the new precoders by utilizing temporal correlations that are captured from past observed precoders (via $f_P$).
We reuse the $q_P$ algorithm from \cite{Gupt1905:Predictive,6891198}.
However, by incorporating the $q_P$ in the reservoir framework, $f_P$ and $q_P$ get coupled in the sense that $f_P$ is optimized via online training of the reservoir by minimizing the norm difference between $\bP_{i,t}=f_P(\cdot)$ and $\tilde{\bU}_{i,t}=q_P(f_P(\cdot),\text{fb}_{i,t})$.
Previous work \cite{Gupt1905:Predictive,6891198} has considered $f_P$ and $q_P$ to be uncoupled with each other.
A detailed discussion on the reservoir computing based model for $f_P$ and $q_P$ is presented in Section \ref{section3}.
The proposed reservoir computing scheme is evaluated against the temporal predictive quantization scheme in \cite{6891198}, and the simulation results are discussed in Section \ref{section4}.

%Explain prediction and quantization steps here itself ! (succintly)
\section{\textbf{Reservoir model for Predictive Quantization}}
\label{section3}
\subsection{\textbf{Vectorizing matrices in Stiefel Manifold}}
\label{vectorize}
The reservoir computing framework has inputs and outputs as real vectors, and thus, it is necessary to convert the precoding matrices to vectors.
A precoding matrix $\mathcal{\bM} \in \text{St}(N_T,N_R)$ is a $\mathbb{C}^{N_T \times N_R}$ matrix with the property $\mathcal{\bM}^H\mathcal{\bM}=\text{\textbf{1}}_{N_R}$.
For brevity, we call such a matrix $\mathcal{\bM}$ a `semi-unitary' matrix.
A na{\"i}ve method to vectorize a $\mathbb{C}^{N_T \times N_R}$ semi-unitary matrix would be to take all its $N_TN_R$ complex numbers individually and stack them onto a $2N_TN_R$ dimensional real vector.
However, a more efficient approach would be to use the $\mathcal{\bM}^H\mathcal{\bM}=\text{\textbf{1}}_{N_R}$ property, which implies that each vector that represents the columns of $\mathcal{\bM}$ is orthogonal to the other columns.
For the first column, we take its $N_T$ complex numbers and stack them onto a $2N_T$ vector.
For the second column, we take only the first $N_T-1$ complex numbers and concatenate, since the last number can be determined by utilizing orthogonality with the first column.
For the third column, we take only the first $N_T-2$ complex numbers, and so on.
Therefore, we vectorize a $\mathbb{C}^{N_T \times N_R}$ semi-unitary matrix into a $2(N_T)+2(N_T-1)+\ldots+2(N_T-N_R+1)= 2N_TN_R-N_R^2+N_R$ dimensional vector.

Observe here that we incur a dimensional penalty upon vectorizing semi-unitary matrices.
$\mathcal{\bM} \in \text{St}(N_T,N_R)$, which is a $2N_TN_R-N_R^2$ dimensional manifold, whereas we get a $2N_TN_R-N_R^2+N_R$ dimensional vector representation.
This corresponds to an overhead of $N_R$ dimensions, since we have not exploited the fact that each of the $N_R$ column vectors is a unit norm vector as well.
It looks as if utilizing the norm 1 property, we could just take $N_T-1$ complex numbers when vectorizing the first column, instead of $N_T$. However, this would lead to non-unique representations, since there could then be infinite possible values that the lone missing complex number can take, so that the vector has unit norm. Hence, we vectorize a semi-unitary $\mathbb{C}^{N_T \times N_R}$ $\mathcal{\bM}$ into $2N_TN_R-N_R^2+N_R$ dimensional vector, denoted as $\bm$.
%Focus on the fact that you don't actually pay a penalty
%The thing to quantize remains on the Stiefel Manifold
\subsection{\textbf{Forward Prediction $f_P$ function}}
\noindent The vector representation of semi-unitary matrices allows to proceed with our discussion on reservoir computing framework.
Consider the reservoir computing framework in Fig. \ref{res_overview} %Ref Fig. Reservoir Pic

The input to the reservoir $i$ that corresponds to subcarrier $i$ is $\tilde{\bu}_{i,t-1}$, (vectorized representation of $\tilde{\bU}_{i,t-1}$) with dimensions $D_{\text{in}}=2N_TN_R-N_R^2+N_R$.
Using a randomly initialized $D_{\text{in}}\times D_{\text{resv}}$ input coupler matrix $\bW^{\text{in}}_i$, the $D_{\text{in}}$ dimensional input is mapped to a $D_{\text{resv}}(>>D_{\text{in}})$ dimensional vector, where $D_{\text{resv}}$ is the dimension of the reservoir state vector $\br_{i,t}$. $\br_{i,t}$ is initialized with $D_{\text{resv}}$ zeros, and is updated via the following equation:
\begin{equation}
\br_{i,t}=\text{tanh}(\bA_i\br_{i,t-1}+\bW^{\text{in}}_i\tilde{\bu}_{i,t-1})
\label{res_upd}
\end{equation}
where $\bA_i$ is the adjacency matrix that captures the reservoir dynamics, and $\text{tanh}$ is applied element wise.
Typical choices for $\bA_i$ have been Erdos-Renyi graphs with an upper bounded maximum eigenvalue \cite{mosleh2017brain,pathak2017using}. The output of the reservoir is $\bp_{i,t}$, a vectorized representation of $\bP_{i,t}$, which denotes the predicted precoder at $i$-th subcarrier,$t$-th time instant.
Therefore, the output of the reservoir is also a $D_{\text{in}}$ dimensional vector ($D_{\text{out}}=D_{\text{in}}$).
The output $\bp_{i,t}$ is obtained using a matrix transformation of the reservoir state $\br_{i,t}$ with the $D_{\text{resv}}\times D_{\text{out}}$ output coupler matrix $\bW^{\text{out}}_{i,t-1}$ viz.
\begin{equation}
\bp_{i,t}=\bW^{\text{out}}_{i,t-1}\br_{i,t}
\label{pred_op}
\end{equation}
\begin{figure*}[ht]
\centering
\includegraphics[width=0.8\textwidth]{images/system2.pdf}
\caption{System Overview: The reservoir computing framework takes the precoder estimate at $t-1$ (viz. $\tilde{\bu}_{i,t-1}$) as input, and executes the forward pass of the predictive quantization framework, shaded in green, to obtain $\tilde{\bu}_{i,t}$. Both the transmitter and receiver utilize the same reservoir (initialized identically), with a minute difference indicated by the cyan shaded block. In the cyan shaded block, receiver finds the appropriate codeword for the $q_P$ function by comparing the prediction with the actual $\bU_{i,t}$, and communicates $\text{fb}_{i,t}$ to allow estimation of $\tilde{\bu}_{i,t}$ at the transmitter. Then, both the receiver and transmitter execute the backward pass (shaded in blue) to optimize $\mathcal{O}_{i,t}$ \eqref{oit}, to ensure identical evolution of both the reservoirs}
\label{res_overview}
\vspace{-5pt}
\end{figure*}
This completes the forward prediction to obtain $\bP_{i,t}$ using only the past observed $\bU_{i,t-1}$ (i.e. $p=1$, Section \ref{section2}). That is,
\begin{equation}
\bP_{i,t}=f_P(\tilde{\bU}_{i,t-1})=\bW^{\text{out}}_{i,t-1}\text{tanh}(\bA_i\br_{i,t-1}+\bW^{\text{in}}_i\tilde{\bu}_{i,t-1})\footnote{For brevity, $\bp_{i,t}$ and $\bP_{i,t}$ are treated to be one and the same}
\label{fwd_pred}
\end{equation}
However, a complete discussion of the framework also involves a backward pass training of the reservoir, involving the updation of $\bW^{\text{out}}_{i,t}$, which would follow subsequently.
\subsection{\textbf{Quantization function $q_P$}}
\label{qp}
We now describe the quantization function $q_P(\bP_{i,t},\text{fb}_{i,t})$, which is similar to the schemes used in \cite{6891198,Gupt1905:Predictive,6545375}.
Central to the quantization algorithm is the fact that tangent spaces local to a point in manifold are linear vector spaces.
Given two points $\bX$ and $\bY$ in $\text{St}(N_T,N_R)$, a lifting operation $\bT^Y_X=\texttt{lift}(\bX,\bY)$, $\texttt{lift}:\text{St}(N_T,N_R)\times \text{St}(N_T,N_R) \to \mathcal{T}_X \text{St}(N_T,N_R)$ yields a tangent from $X$ to $Y$, denoted as $\bT^Y_X \in \mathcal{T}_X \text{St}(N_T,N_R)$, where $\mathcal{T}_X \text{St}(N_T,N_R)$ is the local tangent space at $\bX$.
A corresponding retraction operation $Y=\texttt{retract}(X,\bT^Y_X)$, $\texttt{retract}: \text{St}(N_T,N_R)\times \mathcal{T}_X \text{St}(N_T,N_R) \to \text{St}(N_T,N_R)$ gives back the manifold point obtained by traversing in the tangent direction given by the second argument ($\bT^Y_X$).
In this work, the chosen lifting-retraction pairs are the Cayley exponentials, discussed in \cite{DBLP:journals/corr/abs-1708-00045,Gupt1905:Predictive}. % and used in \cite{Gupt1905:Predictive}.
The Cayley exponential lifting operation maps two points in $\text{St}(N_T,N_R)$, to a $N_T\times N_T$ skew Hermitian matrix that represents the tangent from first point to the other.

The quantization algorithm exploits the linear vector space property of the tangent space and quantizes the local tangent space at the predicted precoder $\bP_{i,t}$, viz $\mathcal{T}_{\bP_{i,t}} \text{St}(N_T,N_R)$.
The codebook for $\mathcal{T}_{\bP_{i,t}} \text{St}(N_T,N_R)$ corresponds to a collection of codewords that represent the different directions (tangents) in $\mathcal{T}_{\bP_{i,t}} \text{St}(N_T,N_R)$.
This codebook is subsequently referred to as the base codebook ($\text{base}^{\mathcal{C}}$) for the quantization function.
The feedback from the receiver indicates the codeword corresponding to the optimum tangent in the $\text{base}^{\mathcal{C}}$, which the transmitter can choose to get closest (in terms of chordal distance metric, $d_s(\bX,\bY)$ for $\bX,\bY \in \text{St}(N_T,N_R)$ detailed in \cite{Gupt1905:Predictive,6891198}) to the actual value $\bU_{i,t}$.
With the optimum direction chosen, the next step is to determine how much to move in that particular chosen direction, viz. the length of the chosen tangent direction.

For this, we adopt the strategy in \cite{6891198}, which controls the magnitude of tangent steps, by having two codebooks $T^{\mathcal{C}}_p,T^{\mathcal{C}}_m$ of different spreads $s_p,s_m$, but same $2^{\mathcal{B}-1}$ base vectors in the $\text{base}^{\mathcal{C}}$, i.e. $T^{\mathcal{C}}_{\{p/m\}}=s_{\{p/m\}}\text{base}^{\mathcal{C}}$ (All codewords $\in \text{base}^{\mathcal{C}}$ multiplied by $s_{\{p/m\}}$ individually).
The two codebooks are concatenated to form a $2^\mathcal{B}$ length codebook $T^\mathcal{C}$.
The receiver finds the optimal index $\text{fb}_{i,t} \in \{1,2,\ldots,2^{\mathcal{B}}\}$ in $T^\mathcal{C}$ by comparing the chordal distance metric ($d_s$), of each codeword to the actual precoder $\bU_{i,t}$ obtained from the SVD of the channel matrix, using \eqref{eqn:cn1}. The receiver then feeds back $\text{fb}_{i,t}$ to the transmitter using $\mathcal{B}$ bits. The transmitter uses the fed back $\text{fb}_{i,t}$ and \eqref{eqn:cn2} to calculate $\tilde{\bU}_{i,t}$,
\begin{equation}
\label{eqn:cn1}
\text{fb}_{i,t} \gets \text{argmin}_{i \in \{1,2,\ldots,2^{\mathcal{B}}\}}\left (d_s\left (\bU_{i,t},\text{retract}(\bP_{i,t},T^\mathcal{C}[i])\right ) \right)
\end{equation}
\begin{equation}
\tilde{\bU}_{i,t}=q_P(\bP_{i,t},\text{fb}_{i,t})=\text{retract}(\bP_{i,t},T^\mathcal{C}[\text{fb}_{i,t}])
\label{eqn:cn2}
\end{equation}
$\text{fb}_{i,t}$ is also used to update the spread of the codebooks $T^{\mathcal{C}}_p,T^{\mathcal{C}}_m$.
Depending on whether $\text{fb}_{i,t} \geq 2^{\mathcal{B}-1}$, i.e. whether the optimum codeword is in $T^{\mathcal{C}}_p$ or $T^{\mathcal{C}}_m$, the scale parameter $s[k]$, which in turn controls values of $s_p, s_m$, is updated as,
$$s_p=g^{\text{min}(s[k-1]+1,0)}, s_m = g^{s[k-1]-1}$$
$$
s[k]=
\begin{cases}
\text{min}(s[k-1]+1,0), \text{ for } \text{fb}_{i,t} \in T^{\mathcal{C}}_p\\
s[k-1]-1, \text{otherwise}\\
\end{cases}
$$
with $s[0]=0$.
Intuitively, the algorithm reduces/increases the spread of the codebook till the reduction/increase of spread is no longer beneficial, i.e. the optimum codeword lies in the higher/lower spread codebook instead.
The scheme used to obtain $\text{base}^{\mathcal{C}}$ is similar to the one presented in \cite{Gupt1905:Predictive}, which performs a $k$-means ($k=2^{\mathcal{B}-1}$) clustering on a collection of tangents to obtain an isotropic collection of $2^{\mathcal{B}-1}$ tangent codewords.
This completes the discussion on the quantization function $q_P$ to obtain $\tilde{\bU}_{i,t}=q_P(\bP_{i,t},\text{fb}_{i,t})$ from $\bP_{i,t},\text{fb}_{i,t}$.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% More information needed about the adjacency
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{\textbf{Reservoir Training Procedure, the backward pass}}
\label{train}
\noindent Having described the technicalities of $f_P,q_P$, we now describe the training process for the reservoir, which couples $f_P$ with $q_P$.
Recall that, to obtain $\tilde{\bU}_{i,t}$ from  $\tilde{\bU}_{i,t-1}$, we have
\begin{equation}
\tilde{\bU}_{i,t}=q_P(\bP_{i,t},\text{fb}_{i,t})=q_P(f_P(\tilde{\bU}_{i,t-1}),\text{fb}_{i,t})
\label{fpqp}
\end{equation}
The key idea for reservoir training is that, as the predicted matrix $\bP_{i,t}$ (the coarse estimate), gets closer to $\tilde{\bU}_{i,t}$ (the refined estimate), the receiver has to quantize even smaller subspaces, and can thus provide a more refined estimate from the feedback $\text{fb}_{i,t}$.
Hence, we train the reservoir output coupler such that the following objective function is optimized:
\begin{equation}
\mathcal{O}_{i,t}=\sum_{s=1}^{t}\frac{1}{\lambda^{t-s}}||\bp_{i,t}-\tilde{\bu}_{i,t}||^2=\sum_{s=1}^{t}\frac{1}{\lambda^{t-s}}||\bW_{i,t}^{\text{out}}\br_{i,t}-\tilde{\bu}_{i,t}||^2
\label{oit}
\end{equation}
Here $\lambda>1$ is the history parameter.
We wish to perform the following optimization in order to find the optimum $\bW_{i,t}^{\text{out}}$ from the available estimates till time $t$:
\begin{equation}
\bW_{i,t}^{\text{out}}\gets\text{argmin}_{\bW_{i,t}^{\text{out}}}(\mathcal{O}_{i,t}=\sum_{s=1}^{t}\frac{1}{\lambda^{t-s}}||\bW_{i,t}^{\text{out}}\br_{i,t}-\tilde{\bu}_{i,t}||^2)
\label{getsOpt}
\end{equation}
Computing the gradient $\frac{\partial \mathcal{O}_{i,t}}{\partial \bW_{i,t}^{\text{out}}}$ and setting it to null yields,
\begin{equation}
\bW_{i,t}^{\text{out}}=\frac{\sum_{s=1}^{t}\frac{1}{\lambda^{t-s}}\texttt{outer}(\tilde{\bu}_{i,s},\br_{i,s})}{\sum_{s=1}^{t}\frac{1}{\lambda^{t-s}}||\br_{i,s}||^2}
\label{backpass}
\end{equation}
where $\texttt{outer}(\bx,\by)$ is the outer product between $N_x$ dimensional vector $\bx$ and $N_y$ dimensional vector $\by$ to yield a $N_x\times N_y$ matrix.
Let $\sum_{s=1}^{t}\frac{1}{\lambda^{t-s}}\texttt{outer}(\tilde{\bu}_{i,s},\br_{i,s})$ be $\text{\textbf{num}}_{i,t}$ and $\sum_{s=1}^{t}\frac{1}{\lambda^{t-s}}||\br_{i,s}||^2$ be $\text{den}_{i,t}$, with $\bW_{i,t}^{\text{out}}=\frac{\text{\textbf{num}}_{i,t}}{\text{den}_{i,t}}$.
Observe that
\begin{equation}
\bW_{i,t}^{\text{out}}=\frac{\frac{\text{\textbf{num}}_{i,t-1}}{\lambda}+\texttt{outer}(\tilde{\bu}_{i,t},\br_{i,t})}{\frac{\text{den}_{i,t-1}}{\lambda}+||\br_{i,t}||^2}
\label{wout_upd}
\end{equation}
\eqref{wout_upd} makes training of the reservoir easy to implement with low complexity.
We only need to store the matrix $\text{\textbf{num}}_{i,t-1}$, a number $\text{den}_{i,t-1}$. Then, using the current reservoir state $\br_{i,t}$ and $\tilde{\bu}_{i,t}$ obtained from \eqref{res_upd}, \eqref{fpqp} respectively, $\bW_{i,t}^{\text{out}}$ can be calculated to obtain the predicted coarse estimate, $\tilde{\bu}_{i,t+1}$.
This completes the technical discussion for the reservoir computing based framework for predictive quantization illustrated in Fig. \ref{res_overview}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

To conclude this section, we summarize and highlight the novel aspects of the proposed framework.
The first key advantage of the proposed approach over the past work \cite{Gupt1905:Predictive,6891198,6545375} is that here the prediction function, $f_P$ is coupled with the quantization function, $q_P$.
Past work for predictive quantization has largely kept $f_P$ and $q_P$ uncoupled in the sense that, the obtained precoder is just composite function of $f_P$ and $q_P$, applied to the previous $p$ precoder estimates i.e. $q_P(f_P(\cdot))$ (Section \ref{section2}). In the proposed framework, $f_P$ changes with time due to the update equation of $\bW_{i,t}^{\text{out}}$ \eqref{wout_upd}. Recall that $\bW_{i,t}^{\text{out}}$ update equation is obtained from optimizing \eqref{oit}, which minimizes the norm error between the prediction $\bp_{i,t}$ (the coarse estimate) and the quantized value $\tilde{\bu}_{i,t}$ (the refined estimate). Hence, backward pass ensures that the coarse estimate provided by $f_P$ gets more refined with time, since optimizing $\bW_{i,t}^{\text{out}}$ to minimize $\mathcal{O}_{i,t}$ brings the predicted estimates $\bp_{i,s\leq t}$ closer to the quantized estimates $\tilde{\bu}_{i,s\leq t}$.
The updated $\bW_{i,t}^{\text{out}}$ is then used to obtain $\bP_{i,t+1}$, which is likely to be closer to $\tilde{\bU}_{i,t+1}$, than $\bP_{i,t}$ was to $\tilde{\bU}_{i,t}$, due to optimization of $\mathcal{O}_{i,t}$.
This enables $q_P$ to quantize even smaller subspaces, since the coarse estimate itself has improved, which in turn brings about lower quantization error, and thus improved performance.

% To see this, observe \eqref{update_Wt1} to obtain $\bW^{\text{out}}_{i,t-1}$ from $\tilde{\bU}_{i,t-1}=q_P(f_P(\tilde{\bU}_{i,t-2}),\text{fb}_{i,t-1})$, and \eqref{pred} to obtain $\bP_{i,t+1}$
% \begin{equation}
% \highlight{\bW_{i,t-1}^{\text{out}}}=\frac{\frac{\text{num}_{i,t-2}}{\lambda}+\texttt{outer}(\highlight{q_P}(f_P(\tilde{\bU}_{i,t-2}),\text{fb}_{i,t-1}),\br_{i,t-1})}{\frac{\text{den}_{i,t-2}}{\lambda}+||\br_{i,t-1}||^2}
% \label{update_Wt1}
% \end{equation}
% \begin{equation}
% \highlight{\bP_{i,t}}=f_P(\tilde{\bU}_{i,t-1})=\highlight{\bW^{\text{out}}_{i,t-1}}\text{tanh}(\bA_i\br_{i,t-1}+\bW^{\text{in}}_i\highlight{\tilde{\bu}_{i,t-1}})
% \label{pred}
% \end{equation}

%\textcolor{blue}
{Another advantage of the proposed scheme is the ease of training the reservoir, captured in \eqref{res_upd}.
By storing just one matrix $\text{\textbf{num}}_{i,t}$ and a number $\text{den}_{i,t}$, we capture the entire history of the precoding matrices, with past values weighted by $\lambda$.
This is a departure from previous schemes \cite{Gupt1905:Predictive,6891198,6545375} which store the past $p$ precoders precoders in a $p$ sized cyclic buffer ($p$ also has to be pre-decided).
Reservoir computing, thus proposes an easy to train data centric scheme, which is not typical for data centric ML based schemes.
}

\section{\textbf{Simulation Results}}
\label{section4}

\subsection{\textbf{Simulation setting considered}}
\label{setting}

\noindent The simulations have been performed for the IEEE Pedestrian-A channel, with $N_T=4$, $N_R=2$.  The channel matrices $\bH_{i,t}$ are generated using Jake's model for realistic simulations, from the IT++ library through the python wrapper py-itpp \cite{ViditPy}. 
We compare the results obtained by our framework, with the work presented in \cite{6891198}, which also utilized temporal correlations for predictive quantization.
% The forward prediction function $f_P$ of the proposed framework, has $p$ (Section \ref{section2}) to be $1$ \eqref{fpqp}, whereas the backward pass $p$ for time instant $t$ is $t$ (Observe the framing of $\mathcal{O}_{i,t}$).   
We keep $p=4$ for the $f_P$ presented in \cite{6891198}.
The quantization function $q_P$ is kept to be same, as described in \cite{6545375,6891198,Gupt1905:Predictive} for both the approaches.
Although in both our work and \cite{6891198}, only temporal correlations are exploited, we compare the BER, quantization error and achievable rate results for the complete MIMO-OFDM setting with $N=64$ of subcarriers, and channels are fed back for $8$ evenly spaced subcarriers, indexed starting at 0, viz. $\{0,9,\ldots,63\}$ ($\{9k, k \in \{0,1,\ldots,7\}\}$). Hence, the feedback is available only for $i=\{9k\}$ subcarriers (i.e. transmitter obtains only $\text{fb}_{\{9k\},t}$).
The precoding matrices at subcarriers where feedback is not available ($i\neq\{9k\}$) are estimated via the Cayley Interpolation method presented in \cite{Gupt1905:Predictive}.
The setting is visually illustrated in Fig. \ref{table}.

A 6 bit codebook generated using the Lloyd codebook algorithm for the Stiefel Manifold presented in \cite{6678348} is used for initial feedback ($\tilde{\bU}_{\{9k\},0}$) and differential quantization (i.e. $\bP_{i,t}=\tilde{\bU}_{i,t-1}$) is used for 10 time instances to provide the initial training data for the reservoir and to overcome the initial transient response of the reservoir (also discussed in \cite{mosleh2017brain}).
A 6 bit base codebook is used for the quantization function $q_P$ and is generated according to the method presented in Section \ref{qp}, similar to \cite{Gupt1905:Predictive}.
Hence, both initialization and subsequent feedback, $\text{fb}_{i,t}$ can be encoded using 6 bits (i.e. $\mathcal{B}=6$ Section \ref{section2}).
The bit budget per OFDM frame is thus 48 bits, for total 8 equally spaced fed back subcarriers $\{9k, k \in \{0,1,\ldots,7\}\}$.

For predicting channels at each of the $8$ fed-back subcarriers, we consider $8$ separate reservoirs, each of them evolved separately, with $i\in\{9k, k \in \{0,1,\ldots,7\}\}$ in both the forward pass prediction and quantization ($f_P,q_P$) \eqref{fpqp} and backward pass to update $\bW_{i,t}^{\text{out}}$ \eqref{backpass}. Unlike \cite{Gupt1905:Predictive}, we do not exploit frequency correlations for predictive quantization, and the $8$ reservoirs for prediction of $8$ fed-back subcarriers are kept independent of each other.
All these 8 reservoirs are initialized with $\bA_{i\in\{9k\}}$ being an Erdos-Reny\"i graph, with probability of edge connections being $0.2$, $\lambda=2$, $\bW^{\text{out/in}}_{i\in\{9k\},0}$ are initialized randomly, and $D_{\text{in}}=D_{\text{out}}=14$ (vectorized representation of $4\times2$ semi-unitary matrix, Section \ref{vectorize}) and $D_{\text{resv}}=60$.
% All these 8 reservoirs are initialized with $\bA_{i\in\{9k\}}$ being an Erdos-Reny\"i graph, with 
\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{images/table.pdf}
\caption{The transmitter obtains feedback for $\{0,9,\ldots,63\}$ ($\{9k, k \in \{0,1,\ldots,7\}\}$ subcarriers, and uses $\tilde{\bU}_{i,t-1}$, along with $f_P,q_P$ \eqref{fpqp} to obtain $\tilde{\bU}_{i,t}$, illustrated in the figure via cyan arrows. For each time instant, the transmitter uses the Cayley method to interpolate $\tilde{\bU}_{i,t}$ at $i\neq\{9k\}$ subcarriers.}
\label{table}
\vspace{5pt}
\end{figure}

\subsection{\textbf{Quantization error, BER and Achievable Rate Results}}
\label{res}{}
% To mitigate errors in prediction upon the presence of randomly appearing drastic events in wireless channels, such as sudden interferences and dynamic environments, only 100 channel evolutions are considered for the simulations, and the algorithm is re-starte.
\noindent Averaging is performed over 10 independent channel realizations, to ensure that the simulation results hold in general for the chosen IEEE Pedestrian A channel profile. Therefore, the quantization error, BER and achievable rate results are averaged over 1000 channel instances, with 100 channel evolutions of 10 independent channel realizations.
\begin{figure}
\centering
\includegraphics[width=0.4\textwidth]{images/qtizErr.pdf}
\caption{Quantization error in terms of chordal distance to the actual precoder $\bU_{\{9k\},t}$ observed at the receiver, from the quantized estimate $\tilde{\bU}_{\{9k\},t}$, averaged over 8 fed back subcarriers ($\{9k, k \in \{0,1,\ldots,7\}\}$) and 10 independent channel realizations (normalized Doppler $f_DT_s=10^{-4}$).}
\label{qtiz_err}
\vspace{-10pt}
\end{figure}

From Fig. \ref{qtiz_err} it is evident that the proposed framework is able to reduce the quantization error below what was obtained from the existing temporal correlations based predictive quantizer in \cite{6891198}. In addition, the quantization error plot is much smoother as well, which can be explained from the fact that the reservoir computing approach optimizes $\mathcal{O}_{i,t}$, that captures the long term dynamics of the channel and hence, does not face jittery variations. We simulate (uncoded QPSK) BER performance for all the 64 subcarriers' precoders obtained after interpolation ($\tilde{\bU}_{i\neq\{9k\},t}$) and quantization ($\tilde{\bU}_{\{9k\},t}$). Observe from Fig. \ref{BER} that reservoir computing framework is able to achieve substantial improvements in $\text{E}_\text{b}\text{N}_0$ levels at $\text{BER}\leq 10^{-4}$. Particularly, for $\text{BER}=10^{-5}$ we observe around $5$ dB improvement. 

We hypothesize that the key reason for the improved performance for both quantization error and BER in the reservoir computing approach, is the fact that $f_P,q_P$ are coupled via the backward pass framework (explained towards the end of Section \ref{train}). Also, observe from Fig. \ref{BER} that the proposed framework BER curve (yellow) comes very close to the ideal $8$ feedback curve (green). This can be explained due to the fact that the semi-unitary precoder basically rotates the data vector via its matrix transformation. For QPSK, we get some reasonable error margin for this rotation, which becomes more stringent as the simulated BER is reduced. Hence, we match the ideal curve for high BER region, and for low BER region, as the margin of error reduces, the curves diverge.

\noindent For simulating the achievable rate, we quantize the sigma values ($\Sigma_{i,t}$ from SVD of $\bH_{i,t}$) using a 2 bit vector quantizer ($k$-means), and feed them back to the transmitter for fed back subcarriers ($\{9k, k \in \{0,1,\ldots,7\}\}$). For the non-fed back subcarriers, they are interpolated via a simple convex combination interpolation method as in \cite{Gupt1905:Predictive}. Observe from Fig. \ref{achievrate} that the reservoir computing framework offers improvement in achievable rate for $f_DT_s=10^{-4}$. Since the temporal variations for $f_DT_s=10^{-2}, 10^{-1}$ are higher, the reservoir computing framework has to learn a faster-varying function, and hence the degradation in performance. However, it still matches the performance of the predictive quantizer in \cite{6891198} for $f_DT_s=10^{-2}$ and performs slightly better for $f_DT_s=10^{-1}$.
\vspace{-3pt}

\begin{figure}
\centering
\includegraphics[width=0.43\textwidth]{images/BER_res.pdf}
\vspace{-5pt}
\caption{BER results: Here, red curve is the most ideal case considering exact $\bU_{i,t}$ for all 64 subcarriers, at each time instant are available at the transmitter. The \{green/yellow/blue\} curves corresponds to the case in which \{$\bU_{9k,t}$/$\tilde{\bU}_{9k,t}$ from reservoir method/$\tilde{\bU}_{9k,t}$ from \cite{6891198}\} is available for each time instant, and non fed back precoders are interpolated via Cayley method. Observe that the proposed framework BER curve (provides significant improvement in $\text{E}_\text{b}\text{N}_0$ levels over the work presented in \cite{6891198} (blue).}
\label{BER}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.43\textwidth]{images/hopRate.pdf}
\vspace{-5pt}
\caption{The results are presented as a percentage of the ideal achievable rate obtained when the transmitter has the accurate (unquantized) and non-interpolated estimates of $\bU_{i,t}$ for each subcarrier and each time instant.}
\label{achievrate}
\vspace{-10pt}
\end{figure}
%Qtisn error with 1000 chan evols
%Qtisn error with 100 chan evols
%BER
%Achievable rat

\section{\textbf{Conclusions and Future Work}}
\label{section5}

In this paper, we have presented a reservoir computing framework for predictive quantization of SVD precoders.
The simulations reveal significant improvement in quantization error, BER, and achievable rate for IEEE Pedestrian-A channel model with normalized Doppler $10^{-4}$. 
The novelty enabling these improvements lies in the training method for the reservoir framework, which refines the predicted coarse estimates as the reservoir evolves.
If the simulation results hold in the hardware implementation, the proposed framework would be able to communicate at significantly lower power levels for the same BER, as illustrated in Fig. \ref{BER}.
Note that these results have been obtained by exploiting just the temporal correlations for predictive quantization, and can potentially improve if temporal-frequency correlations are exploited jointly.
This motivates further study into the reservoir framework, and a detailed analysis of impact on performance caused by choice of different reservoir parameters like $\bA,\text{tanh}(\cdot),$ and $D_{\text{resv}}$.
\vspace{-4pt}

% \section{Acknowledgment}
% % \label {section6}
% % \input{sections/6_section.tex}
% Parts of this work was supported by the Bharti Centre for Communication in
% IIT Bombay, and the Visvesvaraya
% PhD Scheme of Ministry of Electronics \& Information Technology,
% Government of India, being implemented by Digital India Corporation.


\renewcommand{\bibfont}{\footnotesize}
\bibliography{IEEEabrv,main}
\bibliographystyle{IEEEtran}
\end{document}

% Remove the first para of FSM. State what r1,r2,d,\theta are in the FSM model states themselves. As a concluding statement of STAT-2 state that using the
% inherit geometery of the problem, we get AoA
